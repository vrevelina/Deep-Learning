{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Bidirectional LSTM for Sentiment Analysis on IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import utils\n",
    "import time\n",
    "device= torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num review in train =  25000\n",
      "num review in test =  25000\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load('imdb/train_data.pt')\n",
    "train_label = torch.load('imdb/train_label.pt')\n",
    "test_data = torch.load('imdb/test_data.pt')\n",
    "test_label = torch.load('imdb/test_label.pt')\n",
    "\n",
    "print('num review in train = ', len(train_data))\n",
    "print('num review in test = ', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the network. Using a bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rec_neural_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rec_layer = nn.LSTM(hidden_size,hidden_size,num_layers=num_layers,bidirectional=True)\n",
    "        self.lin_layer = nn.Linear(hidden_size*2,output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        \n",
    "        input_seq_emb = self.emb_layer(input_seq)\n",
    "    \n",
    "        output_seq,(h_last,c_last) = self.rec_layer(input_seq_emb)\n",
    "        \n",
    "        h_direc_1  = h_last[2,:,:]\n",
    "        h_direc_2  = h_last[3,:,:]\n",
    "        h_direc_12 = torch.cat( (h_direc_1, h_direc_2)  , dim=1) \n",
    "        \n",
    "        scores = self.lin_layer(h_direc_12)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_neural_net(\n",
      "  (emb_layer): Embedding(25002, 50)\n",
      "  (rec_layer): LSTM(50, 50, num_layers=2, bidirectional=True)\n",
      "  (lin_layer): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n",
      "There are 1351902 (1.35 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 25002\n",
    "num_layers = 2\n",
    "hid_size = 50\n",
    "out_size = 2\n",
    "\n",
    "net = rec_neural_net(vocab_size,hid_size,out_size,num_layers)\n",
    "\n",
    "# SEND NETWORK TO GPU:\n",
    "net = net.to(device)\n",
    "\n",
    "print(net)\n",
    "utils.display_num_param(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1)\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(0,25000-bs,bs):\n",
    "\n",
    "            # extract the minibatch\n",
    "            indices = torch.arange(i,i+bs)            \n",
    "            minibatch_data, minibatch_label =  utils.make_minibatch(indices, test_data, test_label) \n",
    "            \n",
    "            # truncate if review longer than 500:\n",
    "            if minibatch_data.size(0)>500:\n",
    "                minibatch_data = minibatch_data[0:499]  \n",
    "                \n",
    "            # send to GPU    \n",
    "            minibatch_data = minibatch_data.to(device)\n",
    "            minibatch_label = minibatch_label.to(device) \n",
    "            \n",
    "            # feed it to the network\n",
    "            scores=net(minibatch_data) \n",
    "\n",
    "            # compute the error made on this batch\n",
    "            error = utils.get_error( scores , minibatch_label)\n",
    "\n",
    "            # add it to the running error\n",
    "            running_error += error.item()\n",
    "\n",
    "            num_batches+=1\n",
    "\n",
    "    # compute error rate on the full test set\n",
    "    total_error = running_error/num_batches\n",
    "\n",
    "    print( 'error rate on test set =', total_error*100 ,'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 16 passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 \t time= 1.0688046177228292 \t loss= 0.6922096460293501 \t error= 47.628205128205124 percent\n",
      "error rate on test set = 45.03605769230769 percent\n",
      " \n",
      "epoch= 1 \t time= 2.4563684264818826 \t loss= 0.6783428499331841 \t error= 42.70432692307693 percent\n",
      "error rate on test set = 40.22035256410256 percent\n",
      " \n",
      "epoch= 2 \t time= 3.8757243355115256 \t loss= 0.662305282935118 \t error= 39.142628205128204 percent\n",
      "error rate on test set = 36.97115384615385 percent\n",
      " \n",
      "epoch= 3 \t time= 5.304314347108205 \t loss= 0.6393880152549499 \t error= 36.08173076923077 percent\n",
      "error rate on test set = 31.58253205128205 percent\n",
      " \n",
      "epoch= 4 \t time= 6.69748762845993 \t loss= 0.630812076269052 \t error= 35.797275641025635 percent\n",
      "error rate on test set = 34.83173076923077 percent\n",
      " \n",
      "epoch= 5 \t time= 8.083333082993825 \t loss= 0.6077810445657144 \t error= 32.96073717948718 percent\n",
      "error rate on test set = 44.607371794871796 percent\n",
      " \n",
      "epoch= 6 \t time= 9.527599668502807 \t loss= 0.5901870279740065 \t error= 31.254006410256412 percent\n",
      "error rate on test set = 31.318108974358978 percent\n",
      " \n",
      "epoch= 7 \t time= 10.99085408449173 \t loss= 0.5619044397885983 \t error= 28.89823717948718 percent\n",
      "error rate on test set = 30.364583333333332 percent\n",
      " \n",
      "epoch= 8 \t time= 12.452323985099792 \t loss= 0.5131627983771838 \t error= 24.83173076923077 percent\n",
      "error rate on test set = 28.317307692307693 percent\n",
      " \n",
      "epoch= 9 \t time= 13.931970969835918 \t loss= 0.5173891749901649 \t error= 25.240384615384613 percent\n",
      "error rate on test set = 29.310897435897438 percent\n",
      " \n",
      "epoch= 10 \t time= 15.431646994749705 \t loss= 0.46689024353638675 \t error= 21.754807692307693 percent\n",
      "error rate on test set = 21.52644230769231 percent\n",
      " \n",
      "epoch= 11 \t time= 16.902090855439504 \t loss= 0.46446342919117367 \t error= 22.103365384615383 percent\n",
      "error rate on test set = 21.458333333333332 percent\n",
      " \n",
      "epoch= 12 \t time= 18.37191168864568 \t loss= 0.43155544702059184 \t error= 19.635416666666668 percent\n",
      "error rate on test set = 22.952724358974358 percent\n",
      " \n",
      "epoch= 13 \t time= 19.77348394393921 \t loss= 0.4101111616079624 \t error= 18.653846153846153 percent\n",
      "error rate on test set = 23.008814102564102 percent\n",
      " \n",
      "epoch= 14 \t time= 21.22745759487152 \t loss= 0.39464978101925974 \t error= 17.864583333333332 percent\n",
      "error rate on test set = 20.592948717948715 percent\n",
      " \n",
      "epoch= 15 \t time= 22.72000937064489 \t loss= 0.36462755711414874 \t error= 15.997596153846153 percent\n",
      "error rate on test set = 21.310096153846153 percent\n",
      " \n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(16):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "    \n",
    "    shuffled_indices=torch.randperm(25000)\n",
    " \n",
    "    for count in range(0,25000-bs,bs):\n",
    "      \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the minibatch\n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data, minibatch_label =  utils.make_minibatch(indices, train_data, train_label) \n",
    "        \n",
    "        # truncate if review longer than 500:\n",
    "        if minibatch_data.size(0)>500:\n",
    "            minibatch_data = minibatch_data[0:500]  \n",
    "            \n",
    "        # send to GPU    \n",
    "        minibatch_data = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device) \n",
    "\n",
    "        # forward the minibatch through the net        \n",
    "        scores = net(minibatch_data)\n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion( scores , minibatch_label) \n",
    "        \n",
    "        # backward pass    \n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the gradient\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer.step()     \n",
    "\n",
    "        \n",
    "        # computing stats\n",
    "        num_batches+=1\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.item()\n",
    "            error = utils.get_error( scores , minibatch_label)\n",
    "            running_error += error.item()\n",
    "                          \n",
    "    # epoch finished:  compute and display stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    total_error = running_error/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    print('epoch=',epoch, '\\t time=', elapsed/60, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    \n",
    "    # compute error on the test set:\n",
    "    eval_on_test_set() \n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save( net.state_dict() , 'trained_parameters_LSTM.pt'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

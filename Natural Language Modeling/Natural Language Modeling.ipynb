{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Modeling -- 2-Layer LSTM on the PTB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will create a 2 layer LSTM and train our network on the PTB dataset to predict which word will come next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 53,074,704 parameters in my network. I set the initial learning schedule as 3.1. Then starting from the fourth epoch, I divide the learning rate by 1.1 in the beginning of each epoch, and divide again by 2.4 at the end of each epoch. When it reaches 11, I divide the learning rate by 4.5 instead of 2.4. Finally, when it exceeds 15, i divide the learning rate by 8. There are two divisions because I find that when I try to combine them, the preplexity becomes worse. I first experimented with different learning rates, anywhere between 1 to 4, and I found that 3.1 works the best given everything else constant. I also tried several learning rate schedules, and I found that the one i described above works best. Then, I tried changing the embedding size and hidden size. I found that if the difference between the two sizes are small, the preplexity becomes worse, if there is no difference between the two sizes, the preplexity becomes better, but it works best when the two are far apart, with emb_size > hid_size, hidden size has to be small, around 100-300. I think changing the embed size makes the most difference in the preplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    }
   ],
   "source": [
    "train_data  =  torch.load('../../data/ptb/train_data.pt')\n",
    "test_data   =  torch.load('../../data/ptb/test_data.pt')\n",
    "\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rec_neural_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , embedding_size  )\n",
    "        self.layer2 = nn.LSTM(      embedding_size , hidden_size, num_layers=2, dropout=0.3 )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size )\n",
    "        \n",
    "        \n",
    "    def forward(self, word_seq, h_init):\n",
    "        \n",
    "        input_seq_emb = self.layer1( word_seq )\n",
    "        output_seq , (h_final, c_last) = self.layer2( input_seq_emb, (h,c) )\n",
    "        scores_seq = self.layer3( output_seq )   \n",
    "        \n",
    "        return scores_seq, (h_final, c_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_neural_net(\n",
      "  (layer1): Embedding(10000, 5237)\n",
      "  (layer2): LSTM(5237, 292, num_layers=2, dropout=0.3)\n",
      "  (layer3): Linear(in_features=292, out_features=10000, bias=True)\n",
      ")\n",
      "There are 62444656 (62.44 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "emb_size = 5237\n",
    "hid_size = 292\n",
    "\n",
    "net=rec_neural_net(emb_size,hid_size)\n",
    "net = net.to(device)\n",
    "\n",
    "print(net)\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0069, -0.0692, -0.0362,  ...,  0.0284,  0.0921,  0.0357],\n",
       "        [-0.0657,  0.0723, -0.0327,  ...,  0.0865,  0.0085,  0.0020],\n",
       "        [-0.0904,  0.0845, -0.0496,  ...,  0.0302, -0.0925,  0.0876],\n",
       "        ...,\n",
       "        [-0.0557,  0.0961,  0.0363,  ...,  0.0889, -0.0958, -0.0670],\n",
       "        [ 0.0714,  0.0149, -0.0260,  ..., -0.0454,  0.0363, -0.0526],\n",
       "        [ 0.0509, -0.0077, -0.0801,  ...,  0.0064,  0.0028, -0.0469]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr = 3.1\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "       \n",
    "        h = torch.zeros(2, bs, hid_size)\n",
    "        c = torch.zeros(2, bs, hid_size)\n",
    "\n",
    "        h=h.to(device)\n",
    "        c=c.to(device)\n",
    "\n",
    "\n",
    "        for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
    "\n",
    "            minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "            minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "\n",
    "            minibatch_data=minibatch_data.to(device)\n",
    "            minibatch_label=minibatch_label.to(device)\n",
    "\n",
    "            scores, (h1,c1) = net( minibatch_data, h )\n",
    "\n",
    "            minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "            scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "\n",
    "            loss = criterion(  scores ,  minibatch_label )    \n",
    "\n",
    "            h=h.detach()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 21.824772119522095 \t lr= 3.1 \t train: exp(loss)= 315.39196579440596\n",
      "test: exp(loss) =  199.25337341190493\n",
      "\n",
      "epoch= 1 \t time= 44.37513518333435 \t lr= 3.1 \t train: exp(loss)= 149.41416013346404\n",
      "test: exp(loss) =  153.65944548640894\n",
      "\n",
      "epoch= 2 \t time= 67.02786040306091 \t lr= 3.1 \t train: exp(loss)= 111.71319887900349\n",
      "test: exp(loss) =  137.05368945267847\n",
      "\n",
      "epoch= 3 \t time= 89.60040307044983 \t lr= 3.1 \t train: exp(loss)= 91.49975215365893\n",
      "test: exp(loss) =  129.1294538112854\n",
      "\n",
      "epoch= 4 \t time= 112.17343044281006 \t lr= 1.174242424242424 \t train: exp(loss)= 77.14474211901053\n",
      "test: exp(loss) =  125.8397793845628\n",
      "\n",
      "epoch= 5 \t time= 134.66523122787476 \t lr= 0.44478879706152424 \t train: exp(loss)= 61.894419116472264\n",
      "test: exp(loss) =  120.09267649739712\n",
      "\n",
      "epoch= 6 \t time= 157.02283310890198 \t lr= 0.16848060494754705 \t train: exp(loss)= 55.152509247831574\n",
      "test: exp(loss) =  117.5755404311114\n",
      "\n",
      "epoch= 7 \t time= 179.5460183620453 \t lr= 0.06381841096497995 \t train: exp(loss)= 52.529627776315124\n",
      "test: exp(loss) =  117.40524299994995\n",
      "\n",
      "epoch= 8 \t time= 202.12552762031555 \t lr= 0.02417364051703786 \t train: exp(loss)= 51.39720746405614\n",
      "test: exp(loss) =  116.61423464590158\n",
      "\n",
      "epoch= 9 \t time= 224.80342936515808 \t lr= 0.009156682014029492 \t train: exp(loss)= 51.06375626966344\n",
      "test: exp(loss) =  116.42100152787535\n",
      "\n",
      "epoch= 10 \t time= 247.4348521232605 \t lr= 0.003468440156829353 \t train: exp(loss)= 50.80730386481095\n",
      "test: exp(loss) =  115.98765181831327\n",
      "\n",
      "epoch= 11 \t time= 270.00719571113586 \t lr= 0.000700694981177647 \t train: exp(loss)= 50.7322441382772\n",
      "test: exp(loss) =  115.93825691736154\n",
      "\n",
      "epoch= 12 \t time= 292.46706080436707 \t lr= 0.00014155454165204988 \t train: exp(loss)= 50.71233242547152\n",
      "test: exp(loss) =  115.73234153217186\n",
      "\n",
      "epoch= 13 \t time= 314.89108300209045 \t lr= 2.8596877101424217e-05 \t train: exp(loss)= 50.71817851830081\n",
      "test: exp(loss) =  115.69577160084094\n",
      "\n",
      "epoch= 14 \t time= 337.3767535686493 \t lr= 5.777146889176609e-06 \t train: exp(loss)= 50.67656625359571\n",
      "test: exp(loss) =  116.07091527781456\n",
      "\n",
      "epoch= 15 \t time= 359.91849875450134 \t lr= 1.1671003816518402e-06 \t train: exp(loss)= 50.7028190916283\n",
      "test: exp(loss) =  116.14331033647086\n",
      "\n",
      "epoch= 16 \t time= 382.5254068374634 \t lr= 1.326250433695273e-07 \t train: exp(loss)= 50.71027150300859\n",
      "test: exp(loss) =  116.0654365345586\n",
      "\n",
      "epoch= 17 \t time= 405.0465934276581 \t lr= 1.50710276556281e-08 \t train: exp(loss)= 50.75290890828101\n",
      "test: exp(loss) =  115.9438038448356\n",
      "\n",
      "epoch= 18 \t time= 427.6086766719818 \t lr= 1.7126167790486475e-09 \t train: exp(loss)= 50.69850507760429\n",
      "test: exp(loss) =  115.74303716122674\n",
      "\n",
      "epoch= 19 \t time= 450.0911021232605 \t lr= 1.9461554307370992e-10 \t train: exp(loss)= 50.68741392648269\n",
      "test: exp(loss) =  116.01108128375934\n",
      "\n",
      "epoch= 20 \t time= 472.5233232975006 \t lr= 2.211540262201249e-11 \t train: exp(loss)= 50.69867579738998\n",
      "test: exp(loss) =  115.85933398788849\n",
      "\n",
      "epoch= 21 \t time= 494.9926960468292 \t lr= 2.513113934319601e-12 \t train: exp(loss)= 50.663683408830266\n",
      "test: exp(loss) =  115.95734363934615\n",
      "\n",
      "epoch= 22 \t time= 517.5575594902039 \t lr= 2.8558112889995466e-13 \t train: exp(loss)= 50.68522169983206\n",
      "test: exp(loss) =  115.35366457530218\n",
      "\n",
      "epoch= 23 \t time= 540.1215498447418 \t lr= 3.245240101135848e-14 \t train: exp(loss)= 50.69242325036303\n",
      "test: exp(loss) =  116.09770486164244\n",
      "\n",
      "epoch= 24 \t time= 562.6849648952484 \t lr= 3.687772842199827e-15 \t train: exp(loss)= 50.68933237802819\n",
      "test: exp(loss) =  116.07129513810361\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(25):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(2, bs, hid_size)\n",
    "    c = torch.zeros(2, bs, hid_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    c=c.to(device)\n",
    "    \n",
    "    for count in range( 0 , 46445-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ] \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        c=c.detach()\n",
    "        h=h.requires_grad_()\n",
    "        c=c.requires_grad_()\n",
    "        \n",
    "        # forward the minibatch through the net        \n",
    "        scores, (h1,c1)  = net( minibatch_data, (h,c) )\n",
    "    \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    if epoch >= 4 and epoch <= 10:\n",
    "        my_lr = my_lr / 2.4\n",
    "    \n",
    "    if epoch > 10 and epoch <= 15:\n",
    "        my_lr = my_lr/4.5\n",
    "        \n",
    "    if epoch > 15:\n",
    "        my_lr = my_lr/8\n",
    "        \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t train: exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the cells below, we will: \n",
    "1. Take a sentence from the ptb test set\n",
    "2. Convert this sentence into a LongTensor using text2tensor function from utils.py\n",
    "3. Feed the sentence to the network\n",
    "4. Price the 30 most likely words that comes after the last word in the sentence according to the network using the show_most_likely_words function from utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices averaging roughly $ N a barrel higher in the third ... \n",
      "\n",
      "94.2%\t quarter\n",
      "0.8%\t half\n",
      "0.6%\t week\n",
      "0.5%\t or\n",
      "0.4%\t consecutive\n",
      "0.4%\t month\n",
      "0.3%\t period\n",
      "0.2%\t day\n",
      "0.2%\t of\n",
      "0.2%\t &\n",
      "0.2%\t and\n",
      "0.1%\t session\n",
      "0.1%\t hour\n",
      "0.1%\t range\n",
      "0.1%\t term\n",
      "0.1%\t year\n",
      "0.1%\t area\n",
      "0.0%\t quarters\n",
      "0.0%\t <eos>\n",
      "0.0%\t part\n",
      "0.0%\t level\n",
      "0.0%\t market\n",
      "0.0%\t season\n",
      "0.0%\t game\n",
      "0.0%\t section\n",
      "0.0%\t deficit\n",
      "0.0%\t <unk>\n",
      "0.0%\t standard\n",
      "0.0%\t sector\n",
      "0.0%\t third\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"prices averaging roughly $ N a barrel higher in the third\" # taken from ptb dataset\n",
    "h = torch.zeros(2, 1, hid_size)\n",
    "c = torch.zeros(2, 1, hid_size)\n",
    "h = h.to(device)\n",
    "c = c.to(device)\n",
    "\n",
    "x = utils.text2tensor(sentence1)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence1, '... \\n')\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i think my line has been very consistent mrs. hills said at a news ... \n",
      "\n",
      "67.8%\t conference\n",
      "5.6%\t that\n",
      "1.8%\t meeting\n",
      "1.5%\t agency\n",
      "1.0%\t and\n",
      "0.9%\t show\n",
      "0.6%\t <unk>\n",
      "0.6%\t moment\n",
      "0.6%\t firm\n",
      "0.5%\t here\n",
      "0.5%\t price\n",
      "0.3%\t where\n",
      "0.3%\t note\n",
      "0.3%\t in\n",
      "0.3%\t news\n",
      "0.3%\t point\n",
      "0.3%\t cost\n",
      "0.3%\t policy\n",
      "0.2%\t network\n",
      "0.2%\t <eos>\n",
      "0.2%\t group\n",
      "0.2%\t with\n",
      "0.2%\t rate\n",
      "0.2%\t hearing\n",
      "0.2%\t '\n",
      "0.2%\t official\n",
      "0.2%\t panel\n",
      "0.2%\t of\n",
      "0.2%\t research\n",
      "0.2%\t said\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"i think my line has been very consistent mrs. hills said at a news\"\n",
    "h = torch.zeros(2, 1, hid_size)\n",
    "c = torch.zeros(2, 1, hid_size)\n",
    "h = h.to(device)\n",
    "c = c.to(device)\n",
    "\n",
    "x = utils.text2tensor(sentence2)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence2, '... \\n')\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this appears particularly true at gm which had strong sales in the ... \n",
      "\n",
      "12.3%\t u.s.\n",
      "5.9%\t quarter\n",
      "4.8%\t company\n",
      "3.9%\t <unk>\n",
      "2.9%\t past\n",
      "2.6%\t first\n",
      "1.9%\t third\n",
      "1.6%\t region\n",
      "1.5%\t bay\n",
      "1.4%\t industry\n",
      "1.3%\t automotive\n",
      "1.3%\t latest\n",
      "1.1%\t field\n",
      "1.0%\t country\n",
      "0.9%\t markets\n",
      "0.9%\t next\n",
      "0.8%\t market\n",
      "0.8%\t wake\n",
      "0.7%\t new\n",
      "0.7%\t fourth\n",
      "0.7%\t oil\n",
      "0.6%\t area\n",
      "0.6%\t 1990s\n",
      "0.6%\t san\n",
      "0.6%\t world\n",
      "0.6%\t european\n",
      "0.6%\t business\n",
      "0.5%\t midwest\n",
      "0.5%\t period\n",
      "0.5%\t year\n"
     ]
    }
   ],
   "source": [
    "sentence3 = \"this appears particularly true at gm which had strong sales in the\"\n",
    "h = torch.zeros(2, 1, hid_size)\n",
    "c = torch.zeros(2, 1, hid_size)\n",
    "h = h.to(device)\n",
    "c = c.to(device)\n",
    "\n",
    "x = utils.text2tensor(sentence3)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence3, '... \\n')\n",
    "utils.show_most_likely_words(p)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some analysts expect oil prices to remain relatively ... \n",
      "\n",
      "19.0%\t low\n",
      "10.3%\t higher\n",
      "10.1%\t strong\n",
      "10.0%\t high\n",
      "2.0%\t flat\n",
      "1.2%\t more\n",
      "1.2%\t slow\n",
      "1.2%\t <unk>\n",
      "1.1%\t small\n",
      "1.0%\t greater\n",
      "1.0%\t bullish\n",
      "1.0%\t weak\n",
      "1.0%\t lower\n",
      "0.9%\t modest\n",
      "0.9%\t thin\n",
      "0.8%\t positive\n",
      "0.8%\t weaker\n",
      "0.7%\t tight\n",
      "0.7%\t profitable\n",
      "0.7%\t volatile\n",
      "0.6%\t minor\n",
      "0.6%\t favorable\n",
      "0.5%\t better\n",
      "0.5%\t growth\n",
      "0.5%\t difficult\n",
      "0.4%\t good\n",
      "0.4%\t significant\n",
      "0.4%\t well\n",
      "0.4%\t relatively\n",
      "0.3%\t hard\n"
     ]
    }
   ],
   "source": [
    "sentence4 = \"some analysts expect oil prices to remain relatively\"\n",
    "\n",
    "x = utils.text2tensor(sentence4)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence4, '... \\n')\n",
    "utils.show_most_likely_words(p)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a 3 sentences of your own (they should related to economy) and see what the network predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the economy is far from full employment , job creation is ... \n",
      "\n",
      "10.1%\t n't\n",
      "6.2%\t <unk>\n",
      "5.7%\t a\n",
      "3.3%\t to\n",
      "3.2%\t the\n",
      "2.4%\t not\n",
      "2.2%\t being\n",
      "2.0%\t that\n",
      "1.7%\t likely\n",
      "1.7%\t expected\n",
      "1.0%\t slowing\n",
      "1.0%\t in\n",
      "1.0%\t very\n",
      "0.9%\t due\n",
      "0.9%\t an\n",
      "0.8%\t so\n",
      "0.8%\t more\n",
      "0.7%\t growing\n",
      "0.7%\t one\n",
      "0.6%\t still\n",
      "0.6%\t getting\n",
      "0.6%\t too\n",
      "0.5%\t relatively\n",
      "0.5%\t made\n",
      "0.5%\t less\n",
      "0.5%\t often\n",
      "0.5%\t high\n",
      "0.5%\t part\n",
      "0.4%\t rising\n",
      "0.4%\t far\n"
     ]
    }
   ],
   "source": [
    "sentence5 = \"the economy is far from full employment , job creation is\"\n",
    "\n",
    "x = utils.text2tensor(sentence5)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence5, '... \\n')\n",
    "utils.show_most_likely_words(p)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increasing share of government spending has been for transfer payments , rather than for purchases of goods and ... \n",
      "\n",
      "18.9%\t services\n",
      "3.6%\t other\n",
      "3.3%\t equipment\n",
      "3.0%\t the\n",
      "2.3%\t costs\n",
      "1.4%\t that\n",
      "1.3%\t to\n",
      "1.1%\t <unk>\n",
      "1.1%\t service\n",
      "1.1%\t expenses\n",
      "1.0%\t a\n",
      "1.0%\t income\n",
      "1.0%\t at\n",
      "0.9%\t property\n",
      "0.9%\t for\n",
      "0.8%\t companies\n",
      "0.8%\t investment\n",
      "0.8%\t paper\n",
      "0.8%\t industry\n",
      "0.8%\t management\n",
      "0.7%\t sales\n",
      "0.7%\t it\n",
      "0.7%\t loans\n",
      "0.7%\t money\n",
      "0.7%\t business\n",
      "0.7%\t exports\n",
      "0.6%\t construction\n",
      "0.6%\t investments\n",
      "0.6%\t stock\n",
      "0.5%\t was\n"
     ]
    }
   ],
   "source": [
    "sentence6 = \"increasing share of government spending has been for transfer payments , rather than for purchases of goods and\"\n",
    "\n",
    "x = utils.text2tensor(sentence6)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence6, '... \\n')\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich earn higher incomes because they contribute more to society than others do. However, because of diminishing marginal utility, they don't get much value from their last few dollars of ... \n",
      "\n",
      "11.8%\t <unk>\n",
      "3.8%\t their\n",
      "3.4%\t the\n",
      "1.4%\t N\n",
      "1.3%\t health\n",
      "1.2%\t work\n",
      "1.0%\t people\n",
      "1.0%\t research\n",
      "0.9%\t a\n",
      "0.7%\t damage\n",
      "0.7%\t revenue\n",
      "0.6%\t equipment\n",
      "0.6%\t insurance\n",
      "0.5%\t u.s.\n",
      "0.5%\t workers\n",
      "0.5%\t money\n",
      "0.5%\t tax\n",
      "0.5%\t each\n",
      "0.5%\t land\n",
      "0.4%\t space\n",
      "0.4%\t mortgage\n",
      "0.4%\t medical\n",
      "0.4%\t home\n",
      "0.4%\t time\n",
      "0.4%\t business\n",
      "0.3%\t american\n",
      "0.3%\t dollars\n",
      "0.3%\t advertising\n",
      "0.3%\t real\n",
      "0.3%\t sales\n"
     ]
    }
   ],
   "source": [
    "sentence7 = \"The rich earn higher incomes because they contribute more to society than others do. However, because of diminishing marginal utility, they don't get much value from their last few dollars of\"\n",
    "\n",
    "x = utils.text2tensor(sentence7)\n",
    "x = x.view(-1,1)\n",
    "x = x.to(device)\n",
    "\n",
    "scores, (h1,c1)= net(x, (h,c))\n",
    "p = F.softmax(scores[scores.size()[0]-1], dim=1)\n",
    "print(sentence7, '... \\n')\n",
    "utils.show_most_likely_words(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
